# =============================================================================
# Full-Stack Evaluation — All Tiers (L1 → L4 + RAG)
# =============================================================================
# Demonstrates every scorer tier running against the same dataset with an
# OpenAI provider for live inference, W&B tracking, and numerical guard.
# Requires: OPENAI_API_KEY, WANDB_API_KEY
# =============================================================================

name: "full-stack-qa-eval"
description: >
  End-to-end evaluation covering deterministic rules (L1), statistical
  similarity (L2), LLM-as-judge quality (L3), system performance (L4),
  and RAG faithfulness. Use this as a comprehensive regression gate.

provider:
  provider_type: openai
  model_name: "gpt-4o"
  api_key: "${OPENAI_API_KEY}"
  temperature: 0.0
  max_tokens: 2048
  enable_logprobs: true
  top_logprobs: 10
  timeout_seconds: 120

data_source:
  source_type: jsonl
  path: "data/samples/golden_qa.jsonl"

scorers:
  # ── L1: Deterministic format checks ──
  - tier: L1
    scorer_name: length
    min_length: 5
    max_length: 4000
    threshold: 1.0

  - tier: L1
    scorer_name: keyword
    expected_keywords: []
    forbidden_keywords: ["error", "undefined", "NaN", "FIXME"]
    case_sensitive: false
    threshold: 1.0

  - tier: L1
    scorer_name: regex
    pattern: "^[A-Z]"  # Must start with a capital letter
    threshold: 1.0

  # ── L2: Statistical similarity ──
  - tier: L2
    scorer_name: rouge
    rouge_types: ["rouge1", "rouge2", "rougeL"]
    threshold: 0.3

  - tier: L2
    scorer_name: bertscore
    bertscore_model: "roberta-large"
    threshold: 0.75

  - tier: L2
    scorer_name: cosine
    embedding_model: "all-MiniLM-L6-v2"
    threshold: 0.6

  # ── L3: LLM-as-Judge ──
  - tier: L3
    scorer_name: geval
    evaluation_model: "gpt-4o"
    criteria: >
      Is the answer factually correct, well-reasoned, and free of
      hallucinations? Does it directly address the question?
    threshold: 0.6
    include_reason: true

  # ── L4: System performance ──
  - tier: L4
    scorer_name: ttft
    ttft_threshold_ms: 400
    threshold: 0.9

  - tier: L4
    scorer_name: latency_p99
    p99_threshold_ms: 3000
    threshold: 0.8

  - tier: L4
    scorer_name: throughput
    throughput_min_tps: 20.0
    threshold: 0.8

  - tier: L4
    scorer_name: numerical_stability
    sigma_threshold: 3.0
    window_size: 50
    threshold: 0.9

  # ── RAG: Retrieval quality ──
  - tier: RAG
    scorer_name: rag_triple_check
    evaluation_model: "gpt-4o"
    faithfulness_threshold: 0.7
    relevancy_threshold: 0.7
    precision_threshold: 0.7
    threshold: 0.7

numerical_guard:
  enabled: true
  sigma_threshold: 3.0
  enable_nan_check: true
  enable_inf_check: true
  window_size: 50

wandb_project: "aero-eval-full-stack"
wandb_tags: ["full-stack", "regression-gate", "gpt-4o"]
concurrency: 10
fail_fast: false
